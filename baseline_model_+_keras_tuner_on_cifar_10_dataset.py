# -*- coding: utf-8 -*-
"""Baseline model + Keras tuner on Cifar-10 dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/161B7fWr-1oM7Mir7ewPJTdvju8PRWa3M
"""

import os
import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.regularizers import l2
from keras.utils.vis_utils import plot_model

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0
# set up image augmentation
datagen = ImageDataGenerator(rotation_range=15, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)
datagen.fit(x_train)
num_classes=10
y_train = tf,keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

def create_model():
  
  inputs = tf.keras.layers.Input(shape=(32,32,3), name='input')
  x = tf.keras.layers.Conv2D(32, (3,3), activation='relu',strides=(1, 1), padding='same')(inputs)
  x = tf.keras.layers.BatchNormalization(axis=-1)(x)
  x = tf.keras.layers.Conv2D(32, (3,3), activation='relu',strides=(1, 1), padding='same')(x)
  x = tf.keras.layers.BatchNormalization(axis=-1)(x)
  x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1, 1))(x)    

  x = tf.keras.layers.Conv2D(64, (3,3), activation='relu',strides=(1, 1), padding='same')(x)
  x = tf.keras.layers.BatchNormalization(axis=-1)(x)
  x = tf.keras.layers.Conv2D(64, (3,3), activation='relu',strides=(1, 1), padding='same')(x)
  x = tf.keras.layers.BatchNormalization(axis=-1)(x)
  x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1, 1))(x)    

  x = tf.keras.layers.Conv2D(128, (3,3), activation='relu', strides=(1, 1), padding='same')(x)
  x = tf.keras.layers.BatchNormalization(axis=-1)(x)
  x = tf.keras.layers.Conv2D(128, (3,3), activation='relu', strides=(1, 1), padding='same')(x)
  x = tf.keras.layers.BatchNormalization(axis=-1)(x)
  x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1))(x)    


  x = tf.keras.layers.Flatten()(x)
  x = tf.keras.layers.Dense(512, activation='relu')(x)
  x = tf.keras.layers.BatchNormalization()(x)
  x = tf.keras.layers.Dropout(0.5)(x)
  outputs = tf.keras.layers.Dense(10, activation='softmax', name='outputs')(x)
  model = tf.keras.Model(inputs=inputs, outputs=outputs)

  return model

model = create_model()
opt = tf.keras.optimizers.Adam(lr=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

history=model.fit_generator(datagen.flow(x_train, y_train, batch_size=128),
                    steps_per_epoch = len(x_train) / 128, epochs=100, validation_data=(x_test, y_test))

#training accuracy
train_acc = model.evaluate(x_train,y_train,batch_size=128)
train_acc

#test accuracy
test_acc = model.evaluate(x_test,y_test,batch_size=128)
test_acc

def plothist(hist):
    plt.plot(hist.history['accuracy'])
    plt.plot(hist.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

plothist(history)
# before val_acc was 0.8927
# strides on maxpooling was giving 0.8973
# strides on conv2d was giving 0.9003

"""Now lets build model for kerastuner and from that obtain best params and train the model"""

pip install -U keras-tuner

from kerastuner import HyperModel

class CNNHyperModel(HyperModel):
    def __init__(self, input_shape, num_classes):
        self.input_shape = input_shape
        self.num_classes = num_classes

    def build(self, hp):

      inputs = tf.keras.layers.Input(shape=(32,32,3), name='input')
      x = tf.keras.layers.Conv2D(32, (3,3), activation='relu',strides=(1, 1), padding='same')(inputs)
      x = tf.keras.layers.BatchNormalization(axis=-1)(x)
      x = tf.keras.layers.Conv2D(32, (3,3), activation='relu',strides=(1, 1), padding='same')(x)
      x = tf.keras.layers.BatchNormalization(axis=-1)(x)
      x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1))(x)
      x = tf.keras.layers.Dropout(0.25)(x)

      x = tf.keras.layers.Conv2D(64, (3,3), activation='relu',strides=(1, 1), padding='same')(x)
      x = tf.keras.layers.BatchNormalization(axis=-1)(x)
      x = tf.keras.layers.Conv2D(64, (3,3), activation='relu',strides=(1, 1), padding='same')(x)
      x = tf.keras.layers.BatchNormalization(axis=-1)(x)
      x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1))(x) 
      x = tf.keras.layers.Dropout(0.25)(x)


      x =  tf.keras.layers.Conv2D(hp.Choice('num_filters', values=[128,162], default=128), (3,3),
                                  activation=hp.Choice('cnn_activation', values=['relu', 'tanh'], default='relu'),
                                  padding=hp.Choice('padding', values=['same', 'valid'], default='same'))(x)
      x = tf.keras.layers.BatchNormalization(axis=-1)(x)
      x = tf.keras.layers.Conv2D(hp.Choice('num_filters', values=[128,162], default=128), (3,3),
                                  activation=hp.Choice('cnn_activation', values=['relu', 'tanh'], default='relu'),
                                  padding=hp.Choice('padding', values=['same', 'valid'], default='same'))(x)
      x = tf.keras.layers.BatchNormalization(axis=-1)(x)
      x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)
      x = tf.keras.layers.Dropout(0.25)(x)


      x = tf.keras.layers.Flatten()(x)
      x = tf.keras.layers.Dense(units=hp.Int('units', min_value=128, max_value=512, step=32, default=512), 
                                activation=hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'], default='relu'))(x)
      x = tf.keras.layers.BatchNormalization()(x)
      x = tf.keras.layers.Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.05, default=0.5))(x)
      outputs = tf.keras.layers.Dense(10, activation='softmax', name='outputs')(x)
      model = tf.keras.Model(inputs, outputs)

      model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=0.00001, max_value=0.001, sampling='LOG', default=0.0001), decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08), 
                    loss='categorical_crossentropy', metrics=['accuracy'])
      return model

"""Architecture is almost same in both baseline and kerastuner model just i have add the 2 more dropout after conv2d layer and not added the strides in last 2 conv2d layers."""

from kerastuner.tuners import (BayesianOptimization, Hyperband, RandomSearch)
import time
seed=1
Max_trials = 2
Execution_per_trials = 2
Bayesian_num_initial_points = 1

def tuners(hpmodel, directory, project_name):
  random_tuner = RandomSearch(hpmodel, objective='val_accuracy', seed=seed, max_trials=Max_trials, executions_per_trial=Execution_per_trials,
                              directory=f"{directory}_randomsearch", project_name=project_name)
  
  hyperband_tuner = Hyperband(hpmodel, max_epochs=10, objective='val_accuracy', seed=seed, executions_per_trial=Execution_per_trials,
                              directory=f"{directory}_hyperband_tuner", project_name=project_name)
  
  bayesian_tuner = BayesianOptimization(hpmodel, objective='val_accuracy', seed=seed, num_initial_points=Bayesian_num_initial_points, 
                                        max_trials=Max_trials, directory=f"{directory}_bayesian", project_name=project_name)
  
  return [random_tuner, hyperband_tuner, bayesian_tuner]



def tuner_evalution(tuner, x_test, x_train, y_test, y_train):
  # overview of task
  tuner.search_space_summary()
  # perform the hyperparameter tuning
  print('start tuning')
  search_start = time.time()
  tuner.search(x_train, y_train, epochs=10, validation_data=(x_test, y_test))
  search_end = time.time()
  elapsed_time = search_start - search_end

  # show the summary of the search
  tuner.results_summary()

  # the best model
  best_model = tuner.get_best_models(num_models=1)[0]

  # evaluate the best model
  loss, accuracy = best_model.evaluate(x_test, y_test)
  return elapsed_time, loss, accuracy

"""Here we had taken all the tuner and ofcourse it is timetaking so only use that tuner which suits best in your problem"""

hypermodel = CNNHyperModel(input_shape=(32,32,3), num_classes=10)

output_dir = ("/content/drive/My Drive/Cifar-10")
tuners = tuners(hypermodel, directory=output_dir, project_name="simple_cnn_tuning")

results = []
for tuner in tuners:
    elapsed_time, loss, accuracy = tuner_evalution(tuner, x_test, x_train, y_test, y_train)

    print(f"Elapsed time = {elapsed_time:10.4f} s, accuracy = {accuracy}, loss = {loss}")

    results.append([elapsed_time, loss, accuracy])
print(results)

"""I had only done 10 epochs for getting good parameters as it was taking long time for more epochs so thats why i had taken only 10 by this also we can get good hyperparameter and from that we can build our model see the code below we can get this by using (get_best_hyperparameters) function"""

hypermodel = CNNHyperModel(input_shape=(32,32,3), num_classes=10)

output_dir = ("/content/drive/My Drive/Cifar-10")
tuners = tuners(hypermodel, directory=output_dir, project_name="simple_cnn_tuning")

for tuner in tuners:
  best_model = tuner.get_best_hyperparameters(num_trials = 1)[0]
  model_2 = tuner.hypermodel.build(best_model)

history_2=model_2.fit_generator(datagen.flow(x_train, y_train, batch_size=128),
                    steps_per_epoch = len(x_train) / 128, epochs=100, validation_data=(x_test, y_test))

"""I had change the model so accuracy will also change i had change the model coz of time purpose in performing tuner so if you will perform with same model as baseline model then for sure you will see the difference in accuracy after performing keras tuner and getting good hyperparameter."""

#training accuracy
train_acc_2 = model_2.evaluate(x_train,y_train,batch_size=128)
train_acc_2

#test accuracy
test_acc_2 = model_2.evaluate(x_test,y_test,batch_size=128)
test_acc_2

plothist(history_2)